{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10316,"status":"ok","timestamp":1686224763881,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"},"user_tz":-330},"id":"nzx9_ryTnyzY","outputId":"3b0388ff-93cf-4f6a-970e-bb3afc7f163a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.22.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (0.0.8)\n","Collecting pygame==2.1.0 (from gym[classic_control])\n","  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pygame\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.0\n"]}],"source":["!pip install gym[classic_control]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCAit9SpyVGj"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import gym"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-CUyQ68AOBP"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"9tZJcooszPzC","executionInfo":{"status":"ok","timestamp":1686227466555,"user_tz":-330,"elapsed":385,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"outputs":[],"source":["class SoftQAgent:\n","    def __init__(self, env, alpha=0.1, gamma=0.99, tau=0.01):\n","        self.env = env\n","        self.alpha = alpha  # learning rate\n","        self.gamma = gamma  # discount factor\n","        self.tau = tau  # soft target update rate\n","        self.obs_dim = env.observation_space.shape[0]\n","        if isinstance(env.action_space, gym.spaces.Discrete):\n","            self.act_dim = env.action_space.n\n","        else:\n","            self.act_dim = env.action_space.shape[0]\n","\n","        #self.act_dim = env.action_space.shape[0]\n","        self.hid1_dim = 64\n","        self.hid2_dim = 64\n","        self.batch_size = 64\n","        self.replay_buffer = []\n","        self.q1_network = self.build_network()\n","        self.q2_network = self.build_network()\n","        self.q1_target_network = self.build_network()\n","        self.q2_target_network = self.build_network()\n","        self.copy_network_weights(self.q1_network, self.q1_target_network)\n","        self.copy_network_weights(self.q2_network, self.q2_target_network)\n","\n","    # Build the Q-value network\n","    def build_network(self):\n","        inputs = tf.keras.layers.Input(shape=(self.obs_dim,))\n","        hid1 = tf.keras.layers.Dense(self.hid1_dim, activation='relu')(inputs)\n","        hid2 = tf.keras.layers.Dense(self.hid2_dim, activation='relu')(hid1)\n","        q_values = tf.keras.layers.Dense(self.act_dim, activation=None)(hid2)\n","        model = tf.keras.models.Model(inputs=inputs, outputs=q_values)\n","        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.alpha))\n","        return model\n","\n","    def get_action(self, obs):\n","        \"\"\"\n","        Returns an action for a given observation using the current policy.\n","        \"\"\"\n","        with torch.no_grad():\n","            obs = torch.FloatTensor(obs).unsqueeze(0)\n","            q1, q2 = self.q1(obs), self.q2(obs)\n","            q = torch.min(q1, q2)\n","            _, argmax = torch.max(q, dim=1)\n","            action = argmax.item()\n","        return action\n","\n","    # Copy the weights from one network to another\n","    def copy_network_weights(self, source_network, target_network):\n","        target_network.set_weights(source_network.get_weights())\n","\n","    # Update the Q-value networks using the soft Q-learning algorithm\n","    def update_networks(self):\n","        # Sample a batch of transitions from the replay buffer\n","        batch = np.array(self.replay_buffer)[np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)]\n","        obs = batch[:, 0]\n","        act = batch[:, 1]\n","        rew = batch[:, 2]\n","        next_obs = batch[:, 3]\n","        done = batch[:, 4]\n","\n","        # Compute the target Q-values using the soft Bellman equation\n","        next_q1_values = self.q1_target_network.predict(next_obs)\n","        next_q2_values = self.q2_target_network.predict(next_obs)\n","        next_q_values = np.minimum(next_q1_values, next_q2_values)\n","        target_q_values = rew + self.gamma * (1 - done) * (next_q_values - self.tau * np.log(next_q_values))\n","\n","        # Update the Q-value networks\n","        self.q1_network.fit(obs, target_q_values, verbose=0)\n","        self.q2_network.fit(obs, target_q_values, verbose=0)\n","\n","        # Update the target Q-value networks using a soft update\n","        q1_weights = np.array(self.q1_network.get_weights())\n","        q1_target_weights = np.array(self.q1_target_network.get_weights())\n","        q1_target_weights = self.tau * q1_weights + (1 - self.tau) * q1_target_weights\n","        self.q1_target_network.set_weights(q1_target_weights)\n","\n","        q2_weights = np.array(self.q2_network.get_weights())\n","        q2_target_weights = np.array(self.q2_target_network.get_weights())\n","        q2_target_weights = self.tau * q2_weights + (1 - self.tau) * q2_target_weights\n","        self.q2_target_network.set_weights(q2_target_weights)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"executionInfo":{"elapsed":858,"status":"error","timestamp":1683090061635,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"},"user_tz":-330},"id":"rqhyu5Uc2la9","outputId":"84134d2d-6fa8-488f-8596-082c099cd16a"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 174ms/step\n"]},{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-533705913e14>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq1_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Take the chosen action and observe the next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Discrete' object has no attribute 'low'"]}],"source":["import gym\n","\n","# Create the environment\n","env = gym.make('CartPole-v1')\n","\n","# Create the Soft Q-learning agent\n","agent = SoftQAgent(env)\n","\n","# Train the agent\n","num_episodes = 1000\n","for episode in range(num_episodes):\n","    obs = env.reset()\n","    done = False\n","    total_reward = 0\n","    while not done:\n","        # Choose an action using the Q-value network and add some exploration noise\n","        action = agent.q1_network.predict(obs[None])[0]\n","        action += np.random.randn(agent.act_dim) * 0.1\n","        action = np.clip(action, env.action_space.low, env.action_space.high)\n","\n","        # Take the chosen action and observe the next state and reward\n","        next_obs, rew, done, _ = env.step(action)\n","\n","        # Add the transition to the replay buffer\n","        agent.replay_buffer.append((obs, action, rew, next_obs, done))\n","\n","        # Update the Q-value networks and target networks\n","        agent.update_networks()\n","\n","        # Update the current observation and total reward\n","        obs = next_obs\n","        total_reward += rew\n","\n","    # Print the total reward for the episode\n","    print(f'Episode {episode + 1}: Total reward = {total_reward}')\n","\n","# Test the agent\n","obs = env.reset()\n","done = False\n","total_reward = 0\n","while not done:\n","    # Choose the action with the highest Q-value\n","    action = agent.q1_network.predict(obs[None])[0]\n","    obs, rew, done, _ = env.step(action)\n","    total_reward += rew\n","env.close()\n","print(f'Test reward = {total_reward}')"]},{"cell_type":"markdown","metadata":{"id":"D7U7LntM39s5"},"source":["This code will train the SoftQAgent on the CartPole-v1 environment for 1000 episodes and then test the trained agent. The output should show the total reward for each episode during training and the test reward at the end. The trained agent should be able to achieve a high test reward, indicating that it has learned a good policy for the CartPole-v1 environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-43XjfX9ZBu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":446},"executionInfo":{"elapsed":420,"status":"error","timestamp":1683090400557,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"},"user_tz":-330},"id":"POjQscwq22Nq","outputId":"d5d7dbe7-64e5-4d27-bcef-126d7eda412c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-bf5aa680a426>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Choose an action using the Soft Q-learning policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Take a step in the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-f90b4fabd40a>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'SoftQAgent' object has no attribute 'q1'"]}],"source":["import gym\n","import numpy as np\n","from collections import deque\n","\n","# Import the SoftQAgent class\n","\n","# Create the environment\n","env = gym.make('CartPole-v1')\n","\n","# Create the Soft Q-learning agent\n","agent = SoftQAgent(env)\n","\n","# Set up the training loop\n","num_episodes = 1000\n","max_steps = 200\n","score_history = deque(maxlen=100)\n","\n","# Train the agent\n","for i in range(num_episodes):\n","    obs = env.reset()\n","    score = 0\n","    for t in range(max_steps):\n","        # Choose an action using the Soft Q-learning policy\n","        action = agent.get_action(obs)\n","\n","        # Take a step in the environment\n","        next_obs, reward, done, info = env.step(action)\n","\n","        # Add the transition to the replay buffer\n","        agent.replay_buffer.append((obs, action, reward, next_obs, done))\n","\n","        # Update the Q-value networks\n","        if len(agent.replay_buffer) >= agent.batch_size:\n","            agent.update_networks()\n","\n","        # Update the score and observation\n","        score += reward\n","        obs = next_obs\n","\n","        # Check if the episode is done\n","        if done:\n","            break\n","\n","    # Add the score to the score history\n","    score_history.append(score)\n","\n","    # Print the episode score and average score over the last 100 episodes\n","    print(f\"Episode {i}: score = {score}, average score = {np.mean(score_history)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1683089136353,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"},"user_tz":-330},"id":"5xxsyxUr7Kka","outputId":"f884c888-046c-4731-c74f-885c3b352a49"},"outputs":[{"data":{"text/plain":["(4,)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["env.observation_space.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1667,"status":"ok","timestamp":1684157972844,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"},"user_tz":-330},"id":"X_EkaOy47hfN","outputId":"87b4d6dd-0ce5-4c3d-a7b4-58f3302cff6e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["import pygame"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"executionInfo":{"elapsed":474,"status":"error","timestamp":1684158008177,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"},"user_tz":-330},"id":"74LbSR4to03i","outputId":"c3582d8b-d700-4d49-a0d1-4c5effdde234"},"outputs":[{"ename":"error","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-7f17edad8c31>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31merror\u001b[0m: video system not initialized"]}],"source":["pygame.init()\n","print(pygame.display.list_modes())"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"0rYemm8f7nFa","executionInfo":{"status":"ok","timestamp":1686242519870,"user_tz":-330,"elapsed":10588,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from collections import deque\n","import gym\n","\n","\n","class replay_buffer(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = deque(maxlen=self.capacity)\n","\n","    def store(self, observation, action, reward, next_observation, done):\n","        observation = np.expand_dims(observation, 0)\n","        next_observation = np.expand_dims(next_observation, 0)\n","        self.memory.append([observation, action, reward, next_observation, done])\n","\n","    def sample(self, batch_size):\n","        batch = random.sample(self.memory, batch_size)\n","        observations, actions, rewards, next_observations, dones = zip(* batch)\n","        return np.concatenate(observations, 0), actions, rewards, np.concatenate(next_observations, 0), dones\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","\n","class soft_q_net(nn.Module):\n","    def __init__(self, observation_dim, action_dim, alpha):\n","        super(soft_q_net, self).__init__()\n","        self.observation_dim = observation_dim\n","        self.action_dim = action_dim\n","        self.alpha = alpha\n","        self.fc1 = nn.Linear(self.observation_dim, 64)\n","        self.fc2 = nn.Linear(64, 256)\n","        self.fc3 = nn.Linear(256, self.action_dim)\n","\n","    def forward(self, observation):\n","        x = self.fc1(observation)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","        return x\n","\n","    def act(self, observation, i):\n","        with torch.no_grad():\n","            q_value = self.forward(observation)\n","            v = self.getV(q_value)\n","            pi_maxent = torch.exp((q_value - v) / self.alpha)\n","            pi_maxent = pi_maxent / pi_maxent.sum(dim=-1, keepdim=True)\n","\n","            if pi_maxent[0][0]==float('nan') or pi_maxent[0][1]==float('nan'):\n","              print(q_value)\n","              #print('q_value: ', q_value, '\\nv: ', v, '\\npi_maxent: ', pi_maxent)\n","              act1 = np.random.uniform(0, 1)\n","              act2 = 1 - act1\n","              pi_maxent[0][0] = act1\n","              pi_maxent[0][1] = act2\n","              dist = torch.distributions.Categorical(pi_maxent)\n","            else:\n","              dist = torch.distributions.Categorical(pi_maxent)\n","\n","            action = dist.sample().item()\n","        return action\n","\n","    def getV(self, q_value):\n","        v = self.alpha * torch.log((1 / self.alpha * q_value).exp().sum(dim=-1, keepdim=True))\n","        return v\n","\n","\n","def train(buffer, target_model, eval_model, gamma, optimizer, batch_size, loss_fn, count, update_freq):\n","    observation, action, reward, next_observation, done = buffer.sample(batch_size)\n","\n","    observation = torch.FloatTensor(observation)\n","    action = torch.LongTensor(action)\n","    reward = torch.FloatTensor(reward)\n","    next_observation = torch.FloatTensor(next_observation)\n","    done = torch.FloatTensor(done)\n","\n","    q_values = eval_model.forward(observation)\n","    next_q_values = target_model.forward(next_observation)\n","    next_v_values = target_model.getV(next_q_values)\n","    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n","    expected_q_value = reward + gamma * (1 - done) * next_v_values.squeeze(-1)\n","\n","    #loss = loss_fn(q_value, expected_q_value.detach())\n","    loss = (expected_q_value.detach() - q_value).pow(2)\n","    loss = loss.mean()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    if count % update_freq == 0:\n","        target_model.load_state_dict(eval_model.state_dict())\n"]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    gamma = 0.99           # discount rate\n","    learning_rate = 1e-4   # learning rate\n","    batch_size = 32\n","    update_freq = 200\n","    capacity = 5000 #50000\n","    render = False\n","    episode = 500 #100000\n","    alpha = 4\n","\n","    env = gym.make('CartPole-v0')\n","    env = env.unwrapped\n","    observation_dim = env.observation_space.shape[0]    # size 4\n","    action_dim = env.action_space.n   # size 2\n","\n","    target_net = soft_q_net(observation_dim, action_dim, alpha)   # initializing target nn\n","    eval_net = soft_q_net(observation_dim, action_dim, alpha)   # initializing evaluation nn\n","    eval_net.load_state_dict(target_net.state_dict())    # loading initialized params (weights and biases) of target nn to eval nn\n","\n","    optimizer = torch.optim.Adam(eval_net.parameters(), lr=learning_rate)   # optimizer\n","    buffer = replay_buffer(capacity)\n","    loss_fn = nn.MSELoss()\n","    count = 0\n","\n","    weight_reward = None\n","    for i in range(episode):\n","        obs = env.reset()\n","        reward_total = 0\n","        if render:\n","            env.render()\n","        while True:\n","            action = eval_net.act(torch.FloatTensor(np.expand_dims(obs, 0)), i)\n","            count += 1\n","            next_obs, reward, done, info, _ = env.step(action)\n","            buffer.store(obs, action, reward, next_obs, done)\n","            reward_total += reward\n","            obs = next_obs\n","            if render:\n","                env.render()\n","            if len(buffer.memory) > batch_size:\n","                train(buffer, target_net, eval_net, gamma, optimizer, batch_size, loss_fn, count, update_freq)\n","\n","            if done:\n","                if not weight_reward:\n","                    weight_reward = reward_total\n","                else:\n","                    weight_reward = 0.99 * weight_reward + 0.01 * reward_total\n","                if (i+1) % 10 == 0:\n","                    print('episode: {}\\treward: {}\\tweight_reward: {:.3f}'.format(i+1, reward_total, weight_reward))\n","                break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"poGurhyZFIDB","executionInfo":{"status":"error","timestamp":1686243348106,"user_tz":-330,"elapsed":773109,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"5bd88e5b-83e0-4e12-df2d-4650d363ff1c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["episode: 10\treward: 25.0\tweight_reward: 68.925\n","episode: 20\treward: 13.0\tweight_reward: 64.342\n","episode: 30\treward: 20.0\tweight_reward: 60.326\n","episode: 40\treward: 31.0\tweight_reward: 56.468\n","episode: 50\treward: 15.0\tweight_reward: 53.019\n","episode: 60\treward: 32.0\tweight_reward: 49.515\n","episode: 70\treward: 23.0\tweight_reward: 46.321\n","episode: 80\treward: 15.0\tweight_reward: 43.614\n","episode: 90\treward: 30.0\tweight_reward: 41.744\n","episode: 100\treward: 18.0\tweight_reward: 40.667\n","episode: 110\treward: 15.0\tweight_reward: 39.465\n","episode: 120\treward: 44.0\tweight_reward: 38.662\n","episode: 130\treward: 23.0\tweight_reward: 38.616\n","episode: 140\treward: 40.0\tweight_reward: 38.103\n","episode: 150\treward: 52.0\tweight_reward: 38.724\n","episode: 160\treward: 31.0\tweight_reward: 42.145\n","episode: 170\treward: 101.0\tweight_reward: 44.296\n","episode: 180\treward: 65.0\tweight_reward: 49.003\n","episode: 190\treward: 144.0\tweight_reward: 57.633\n","episode: 200\treward: 106.0\tweight_reward: 62.742\n","episode: 210\treward: 134.0\tweight_reward: 67.797\n","episode: 220\treward: 181.0\tweight_reward: 74.624\n","episode: 230\treward: 112.0\tweight_reward: 80.810\n","episode: 240\treward: 177.0\tweight_reward: 91.542\n","episode: 250\treward: 317.0\tweight_reward: 100.031\n","episode: 260\treward: 148.0\tweight_reward: 117.392\n","episode: 270\treward: 355.0\tweight_reward: 133.078\n","episode: 280\treward: 381.0\tweight_reward: 151.849\n","episode: 290\treward: 224.0\tweight_reward: 164.165\n","episode: 300\treward: 262.0\tweight_reward: 175.098\n","episode: 310\treward: 584.0\tweight_reward: 212.562\n","episode: 320\treward: 251.0\tweight_reward: 232.386\n","episode: 330\treward: 780.0\tweight_reward: 266.455\n","episode: 340\treward: 198.0\tweight_reward: 291.867\n","episode: 350\treward: 127.0\tweight_reward: 279.534\n","episode: 360\treward: 132.0\tweight_reward: 266.678\n","episode: 370\treward: 224.0\tweight_reward: 259.453\n","episode: 380\treward: 3638.0\tweight_reward: 308.896\n","episode: 390\treward: 278.0\tweight_reward: 303.633\n","episode: 400\treward: 11890.0\tweight_reward: 447.501\n","episode: 410\treward: 4846.0\tweight_reward: 698.541\n","episode: 420\treward: 653.0\tweight_reward: 992.540\n","episode: 430\treward: 356.0\tweight_reward: 1100.616\n","episode: 440\treward: 632.0\tweight_reward: 1207.357\n","episode: 450\treward: 348.0\tweight_reward: 1225.314\n","episode: 460\treward: 666.0\tweight_reward: 1274.649\n","episode: 470\treward: 245.0\tweight_reward: 1405.844\n","episode: 480\treward: 193.0\tweight_reward: 1287.586\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-8f015731ede2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-426db8a8277e>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, observation, i)\u001b[0m\n\u001b[1;32m     61\u001b[0m               \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi_maxent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m               \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi_maxent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     63\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (1, 2)) of distribution Categorical(probs: torch.Size([1, 2])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan]])"]}]},{"cell_type":"code","source":["obs_ = env.reset()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3eqhtcIn8o_q","executionInfo":{"status":"ok","timestamp":1686243630881,"user_tz":-330,"elapsed":17,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"2f7ed6c0-b4cd-4f77-ff16-c5eb2745823b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["q_values = target_net.forward(torch.FloatTensor(np.expand_dims(obs_, 0)))"],"metadata":{"id":"FCG6-o1W8NDC","executionInfo":{"status":"ok","timestamp":1686243658475,"user_tz":-330,"elapsed":714,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["q_values"],"metadata":{"id":"LE2qu7kw9AlG","executionInfo":{"status":"ok","timestamp":1686243660251,"user_tz":-330,"elapsed":11,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"15bcaf39-ab63-44c0-df88-1263c8a077f0","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[342.9336, 340.2656]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7589,"status":"ok","timestamp":1686227722334,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"},"user_tz":-330},"id":"kG-MxSsXp5A_","outputId":"24cefe33-b8ec-4b50-8489-78fe1e6d0282"},"outputs":[{"output_type":"stream","name":"stdout","text":["episode: 10\treward: 14.0\tweight_reward: 14.398\n","episode: 20\treward: 21.0\tweight_reward: 14.642\n","episode: 30\treward: 24.0\tweight_reward: 15.254\n","episode: 40\treward: 14.0\tweight_reward: 15.740\n","episode: 50\treward: 12.0\tweight_reward: 16.231\n","episode: 60\treward: 18.0\tweight_reward: 17.174\n","episode: 70\treward: 33.0\tweight_reward: 17.606\n","episode: 80\treward: 19.0\tweight_reward: 17.399\n","episode: 90\treward: 60.0\tweight_reward: 18.595\n","episode: 100\treward: 20.0\tweight_reward: 19.141\n"]}],"source":["\n","gamma = 0.99           # discount rate\n","learning_rate = 1e-4   # learning rate\n","batch_size = 32\n","update_freq = 200\n","capacity = 50000\n","render = False\n","episode = 100 #100000\n","alpha = 4\n","\n","env = gym.make('CartPole-v0')\n","env = env.unwrapped\n","observation_dim = env.observation_space.shape[0]    # size 4\n","action_dim = env.action_space.n   # size 2\n","\n","target_net = soft_q_net(observation_dim, action_dim, alpha)   # initializing target nn\n","eval_net = soft_q_net(observation_dim, action_dim, alpha)   # initializing evaluation nn\n","eval_net.load_state_dict(target_net.state_dict())    # loading initialized params (weights and biases) of target nn to eval nn\n","\n","optimizer = torch.optim.Adam(eval_net.parameters(), lr=learning_rate)   # optimizer\n","buffer = replay_buffer(capacity)\n","loss_fn = nn.MSELoss()\n","count = 0\n","\n","weight_reward = None\n","for i in range(episode):\n","    obs = env.reset()\n","    reward_total = 0\n","    if render:\n","        env.render()\n","    while True:\n","        action = eval_net.act(torch.FloatTensor(np.expand_dims(obs, 0)), i)\n","        count += 1\n","        next_obs, reward, done, info, _ = env.step(action)\n","        buffer.store(obs, action, reward, next_obs, done)\n","        reward_total += reward\n","        obs = next_obs\n","        if render:\n","            env.render()\n","        if len(buffer.memory) > batch_size:\n","            train(buffer, target_net, eval_net, gamma, optimizer, batch_size, loss_fn, count, update_freq)\n","\n","        if done:\n","            if not weight_reward:\n","                weight_reward = reward_total\n","            else:\n","                weight_reward = 0.99 * weight_reward + 0.01 * reward_total\n","            if (i+1) % 10 == 0:\n","                print('episode: {}\\treward: {}\\tweight_reward: {:.3f}'.format(i+1, reward_total, weight_reward))\n","            break"]},{"cell_type":"code","source":["target_net."],"metadata":{"id":"ZQ0QklTWDiPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXwumfl_SDR8"},"outputs":[],"source":["episode: 320\treward: 213.0\tweight_reward: 209.849\n","\n","episode: 710\treward: 5240.0\tweight_reward: 2442.077"]},{"cell_type":"code","source":["V.dim()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KkWlXvF1GIWa","executionInfo":{"status":"ok","timestamp":1684400649663,"user_tz":-330,"elapsed":509,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"43e72af2-1c89-4a34-fb04-98e67e131dff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":609,"status":"ok","timestamp":1684158918427,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"},"user_tz":-330},"id":"cJQD8zDtqCdw","outputId":"9890f9cb-c14c-4758-eccc-d7ca725549a6"},"outputs":[{"data":{"text/plain":["0.46622402653910955"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["np.random.uniform(0, 1)"]},{"cell_type":"code","source":[],"metadata":{"id":"CSfbQ_sSYzx-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from collections import deque\n","import gym"],"metadata":{"id":"QuwDcVNofCOo","executionInfo":{"status":"ok","timestamp":1686237839342,"user_tz":-330,"elapsed":4299,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["class replay_buffer(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = deque(maxlen=self.capacity)\n","\n","    def store(self, observation, action, reward, next_observation, done):\n","        observation = np.expand_dims(observation, 0)\n","        next_observation = np.expand_dims(next_observation, 0)\n","        self.memory.append([observation, action, reward, next_observation, done])\n","\n","    def sample(self, batch_size):\n","        batch = random.sample(self.memory, batch_size)\n","        observations, actions, rewards, next_observations, dones = zip(* batch)\n","        return np.concatenate(observations, 0), actions, rewards, np.concatenate(next_observations, 0), dones\n","\n","    def __len__(self):\n","        return len(self.memory)"],"metadata":{"id":"7Mjs3oMHe9cc","executionInfo":{"status":"ok","timestamp":1686237845430,"user_tz":-330,"elapsed":6,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["deque?"],"metadata":{"id":"VCr9KRKWfiSQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class soft_q_net(nn.Module):\n","    def __init__(self, observation_dim, action_dim, alpha):\n","        super(soft_q_net, self).__init__()\n","        self.observation_dim = observation_dim\n","        self.action_dim = action_dim\n","        self.alpha = alpha\n","        self.fc1 = nn.Linear(self.observation_dim, 64)\n","        self.fc2 = nn.Linear(64, 256)\n","        self.fc3 = nn.Linear(256, self.action_dim)\n","\n","    def forward(self, observation):\n","        x = self.fc1(observation)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","        return x\n","\n","    def act(self, observation):\n","        with torch.no_grad():\n","            q_value = self.forward(observation)\n","            v = self.getV(q_value)\n","            pi_maxent = torch.exp((q_value - v) / self.alpha)\n","            pi_maxent_ = pi_maxent / pi_maxent.sum(dim=-1, keepdim=True)\n","            dist = torch.distributions.Categorical(pi_maxent_)\n","\n","            action = dist.sample().item()\n","        return q_value, v, pi_maxent, pi_maxent_, dist, action\n","\n","    def getV(self, q_value):\n","        v = self.alpha * torch.log((1 / self.alpha * q_value).exp().sum(dim=-1, keepdim=True))\n","        return v"],"metadata":{"id":"CLwkxPflYzu4","executionInfo":{"status":"ok","timestamp":1686237868266,"user_tz":-330,"elapsed":3,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["torch.no_grad?"],"metadata":{"id":"XqjpqCg-kFa0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gamma = 0.99           # discount rate\n","learning_rate = 1e-4   # learning rate\n","batch_size = 32\n","update_freq = 200\n","capacity = 50000\n","render = False\n","episode = 10 #100000\n","alpha = 4"],"metadata":{"id":"sxlFini_ZcXU","executionInfo":{"status":"ok","timestamp":1686225006919,"user_tz":-330,"elapsed":423,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"070bYJ3QrWtZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686225031339,"user_tz":-330,"elapsed":370,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"a48136aa-da88-4975-86ef-eed2221d3001"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["env = gym.make('CartPole-v0')\n","env = env.unwrapped\n","observation_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n"]},{"cell_type":"code","source":["target_net = soft_q_net(observation_dim, action_dim, alpha)\n","eval_net = soft_q_net(observation_dim, action_dim, alpha)"],"metadata":{"id":"8Hhubla5QOXy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_net?"],"metadata":{"id":"C7ZLgwjRcWM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(target_net.state_dict())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddU-zHqBZlrO","executionInfo":{"status":"ok","timestamp":1684389285656,"user_tz":-330,"elapsed":1052,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"cac0bd75-d94b-466b-9a18-7b3df1dbd2f8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["target_net.state_dict()['fc1.weight'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPRJGRqmZ3tP","executionInfo":{"status":"ok","timestamp":1684389083842,"user_tz":-330,"elapsed":9,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"696f44ab-edcb-4e4a-edd3-02f826f5b57d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 4])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["target_net.state_dict()['fc1.bias'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9moFiucbF8F","executionInfo":{"status":"ok","timestamp":1684389267768,"user_tz":-330,"elapsed":990,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"d3289023-a8a9-4bb6-f8ce-16c6885286ba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["target_net.state_dict()['fc3.weight'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azpyvx8PbP-r","executionInfo":{"status":"ok","timestamp":1684389307984,"user_tz":-330,"elapsed":435,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"21fde78a-a481-4b8a-be09-904abd94a94d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 256])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["target_net.state_dict()['fc3.weight']"],"metadata":{"id":"Lrof6kCldMkp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_net.load_state_dict(target_net.state_dict())\n","#eval_net.state_dict()['fc3.weight']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrp5zxPbdU-R","executionInfo":{"status":"ok","timestamp":1684408137103,"user_tz":-330,"elapsed":526,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"61007517-61c0-485d-9738-7795848eeba2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["for params in eval_net.parameters():\n","  print(params.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56ZUiMfBdnWE","executionInfo":{"status":"ok","timestamp":1684390156150,"user_tz":-330,"elapsed":16,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"bf3ee69c-0ef8-498a-ed3a-e97671d07dae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 4])\n","torch.Size([64])\n","torch.Size([256, 64])\n","torch.Size([256])\n","torch.Size([2, 256])\n","torch.Size([2])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(eval_net.parameters(), lr=learning_rate)"],"metadata":{"id":"ypgqT2iSdGXs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["buffer = replay_buffer(capacity)\n","loss_fn = nn.MSELoss()\n","count = 0"],"metadata":{"id":"3mtJxeO3Q8cC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env.observation_space"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hsQhK59Vjxkp","executionInfo":{"status":"ok","timestamp":1684408324561,"user_tz":-330,"elapsed":6,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"6799769a-6e87-469d-821f-9541741d69d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"execute_result","data":{"text/plain":["Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["env.action_space"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bFRtC1BwV0Z6","executionInfo":{"status":"ok","timestamp":1684408307250,"user_tz":-330,"elapsed":691,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"db566787-733f-46fb-9b41-dfb66b213a1a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discrete(2)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["np.expand_dims(obs, 0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"jdBYF1DIjpia","executionInfo":{"status":"error","timestamp":1684408727058,"user_tz":-330,"elapsed":988,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"427d91ff-7417-400c-b9b0-32a667b264ed"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-a0e2d438b019>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'obs' is not defined"]}]},{"cell_type":"code","source":["env.step?"],"metadata":{"id":"WJMZsLfN_k5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weight_reward = None\n","for i in range(episode)[:1]:\n","    obs = env.reset()\n","    reward_total = 0\n","    if render:\n","      env.render()\n","    while True:\n","      q_value, v, pi_maxent, pi_maxent_, dist, action = eval_net.act(torch.FloatTensor(np.expand_dims(obs, 0)))\n","      print(q_value, '||', v, '||', pi_maxent, '||', pi_maxent_, '||', dist, '||', action, '\\nTotal reward: ', reward_total, 'count: ', count, '\\n-------------------------------------\\n') \n","      count += 1\n","      next_obs, reward, terminated, truncated, info = env.step(action)\n","      buffer.store(obs, action, reward, next_obs, terminated)\n","      reward_total += reward\n","      obs = next_obs    \n","      if terminated:\n","        break\n","      print(count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njDS0ms6V37I","executionInfo":{"status":"ok","timestamp":1684399660112,"user_tz":-330,"elapsed":430,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"e8854ad0-4af1-42fc-b2ce-0d49837b3b21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["q value:  tensor([[0.0759, 0.0228]])\n","tensor([[0.0759, 0.0228]]) || tensor([[2.8220]]) || tensor([[0.5033, 0.4967]]) || tensor([[0.5033, 0.4967]]) || Categorical(probs: torch.Size([1, 2])) || 1 \n","Total reward:  0 count:  1 \n","-------------------------------------\n","\n","2\n","q value:  tensor([[ 0.1231, -0.0118]])\n","tensor([[ 0.1231, -0.0118]]) || tensor([[2.8288]]) || tensor([[0.5084, 0.4916]]) || tensor([[0.5084, 0.4916]]) || Categorical(probs: torch.Size([1, 2])) || 1 \n","Total reward:  1.0 count:  2 \n","-------------------------------------\n","\n","3\n","q value:  tensor([[ 0.1731, -0.0454]])\n","tensor([[ 0.1731, -0.0454]]) || tensor([[2.8379]]) || tensor([[0.5137, 0.4863]]) || tensor([[0.5137, 0.4863]]) || Categorical(probs: torch.Size([1, 2])) || 1 \n","Total reward:  2.0 count:  3 \n","-------------------------------------\n","\n","4\n","q value:  tensor([[ 0.2177, -0.0717]])\n","tensor([[ 0.2177, -0.0717]]) || tensor([[2.8482]]) || tensor([[0.5181, 0.4819]]) || tensor([[0.5181, 0.4819]]) || Categorical(probs: torch.Size([1, 2])) || 0 \n","Total reward:  3.0 count:  4 \n","-------------------------------------\n","\n","5\n","q value:  tensor([[ 0.1741, -0.0457]])\n","tensor([[ 0.1741, -0.0457]]) || tensor([[2.8383]]) || tensor([[0.5137, 0.4863]]) || tensor([[0.5137, 0.4863]]) || Categorical(probs: torch.Size([1, 2])) || 0 \n","Total reward:  4.0 count:  5 \n","-------------------------------------\n","\n","6\n","q value:  tensor([[ 0.1265, -0.0142]])\n","tensor([[ 0.1265, -0.0142]]) || tensor([[2.8293]]) || tensor([[0.5088, 0.4912]]) || tensor([[0.5088, 0.4912]]) || Categorical(probs: torch.Size([1, 2])) || 0 \n","Total reward:  5.0 count:  6 \n","-------------------------------------\n","\n","7\n","q value:  tensor([[0.0803, 0.0192]])\n","tensor([[0.0803, 0.0192]]) || tensor([[2.8225]]) || tensor([[0.5038, 0.4962]]) || tensor([[0.5038, 0.4962]]) || Categorical(probs: torch.Size([1, 2])) || 1 \n","Total reward:  6.0 count:  7 \n","-------------------------------------\n","\n","8\n","q value:  tensor([[ 0.1308, -0.0175]])\n","tensor([[ 0.1308, -0.0175]]) || tensor([[2.8299]]) || tensor([[0.5093, 0.4907]]) || tensor([[0.5093, 0.4907]]) || Categorical(probs: torch.Size([1, 2])) || 1 \n","Total reward:  7.0 count:  8 \n","-------------------------------------\n","\n","9\n","q value:  tensor([[ 0.1807, -0.0501]])\n","tensor([[ 0.1807, -0.0501]]) || tensor([[2.8396]]) || tensor([[0.5144, 0.4856]]) || tensor([[0.5144, 0.4856]]) || Categorical(probs: torch.Size([1, 2])) || 1 \n","Total reward:  8.0 count:  9 \n","-------------------------------------\n","\n","10\n","q value:  tensor([[ 0.2282, -0.0791]])\n","tensor([[ 0.2282, -0.0791]]) || tensor([[2.8501]]) || tensor([[0.5192, 0.4808]]) || tensor([[0.5192, 0.4808]]) || Categorical(probs: torch.Size([1, 2])) || 1 \n","Total reward:  9.0 count:  10 \n","-------------------------------------\n","\n","11\n","q value:  tensor([[ 0.2861, -0.1069]])\n","tensor([[ 0.2861, -0.1069]]) || tensor([[2.8670]]) || tensor([[0.5245, 0.4755]]) || tensor([[0.5245, 0.4755]]) || Categorical(probs: torch.Size([1, 2])) || 1 \n","Total reward:  10.0 count:  11 \n","-------------------------------------\n","\n","12\n","q value:  tensor([[ 0.3549, -0.1333]])\n","tensor([[ 0.3549, -0.1333]]) || tensor([[2.8908]]) || tensor([[0.5305, 0.4695]]) || tensor([[0.5305, 0.4695]]) || Categorical(probs: torch.Size([1, 2])) || 1 \n","Total reward:  11.0 count:  12 \n","-------------------------------------\n","\n","13\n","q value:  tensor([[ 0.4189, -0.1575]])\n","tensor([[ 0.4189, -0.1575]]) || tensor([[2.9137]]) || tensor([[0.5360, 0.4640]]) || tensor([[0.5360, 0.4640]]) || Categorical(probs: torch.Size([1, 2])) || 1 \n","Total reward:  12.0 count:  13 \n","-------------------------------------\n","\n","14\n","q value:  tensor([[ 0.4813, -0.1812]])\n","tensor([[ 0.4813, -0.1812]]) || tensor([[2.9363]]) || tensor([[0.5413, 0.4587]]) || tensor([[0.5413, 0.4587]]) || Categorical(probs: torch.Size([1, 2])) || 0 \n","Total reward:  13.0 count:  14 \n","-------------------------------------\n","\n"]}]},{"cell_type":"code","source":["# both pi_maxent and pi_maxent_ are same"],"metadata":{"id":"eUG_-O4TsuyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count += 1\n","            next_obs, reward, done, info, _ = env.step(action)\n","            buffer.store(obs, action, reward, next_obs, done)\n","            reward_total += reward\n","            obs = next_obs"],"metadata":{"id":"MaQc7PtujW7v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env.render?"],"metadata":{"id":"qYvW9Qk0gpQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BZpe4n6PgstV"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQXkgcNGbWUwBwkDA1jYIw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}