{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHrq6AfAALfjV9KqXY+tj7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qxHgf21fjDVA","executionInfo":{"status":"ok","timestamp":1686740236816,"user_tz":-330,"elapsed":5767,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"08ed9107-9eab-4b06-8bea-8b52f11a371d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.22.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (0.0.8)\n","Collecting pygame==2.1.0 (from gym[classic_control])\n","  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pygame\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.0\n"]}],"source":["!pip install gym[classic_control]"]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from collections import deque\n","import gym"],"metadata":{"id":"Vfl-EJgkjUNa","executionInfo":{"status":"ok","timestamp":1686740276602,"user_tz":-330,"elapsed":4746,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["## Definining SQL Architecture\n","\n","class Soft_Q_Net(nn.Module):\n","  def __init__(self, observation_dim, action_dim, alpha):\n","    super(Soft_Q_Net, self).__init__()\n","    self.alpha = alpha\n","    self.observation_dim = observation_dim\n","    self.action_dim = action_dim\n","    # define network architecture\n","    self.FC1 = nn.Linear(self.observation_dim, 64)    # input layer\n","    self.FC2 = nn.Linear(64, 256)                     # hidden layer\n","    self.FC3 = nn.Linear(256, self.action_dim)        # output layer\n","\n","  # network connecting\n","  def forward_pass(self, observation):\n","    x = self.FC1(observation)\n","    x = F.relu(x)\n","    x = self.FC2(x)\n","    x = F.relu(x)\n","    x = self.FC3(x)\n","    return x\n","\n","  # selecting action\n","  def act(self, observation):\n","    with torch.no_grad():\n","      q_val = self.forward_pass(observation)                        # estimating soft Q value function\n","      v_val = self.get_V_val(q_val)                                  # estimating soft V value function\n","      pi_maxent = torch.exp((q_val - v_val) / self.alpha)\n","      pi_maxent = pi_maxent / pi_maxent.sum(dim=-1, keepdim=True)\n","      pi_maxent[torch.isnan(pi_maxent)] = 1e-6                      # set pi_maxent value to a small value incase numerical instability (Avoid output 'NaN')\n","      policy_dist = torch.distributions.Categorical(pi_maxent)\n","      action = policy_dist.sample().item()                          # sample an action from Categorical distribution\n","    return q_val, v_val, pi_maxent, action\n","\n","  # defining soft V value function\n","  def get_V_val(self, q_value):\n","    v = self.alpha * torch.log((1 / self.alpha * q_value).exp().sum(dim=-1, keepdim=True))\n","    return v"],"metadata":{"id":"gmhoVZXxjf83","executionInfo":{"status":"ok","timestamp":1686740314746,"user_tz":-330,"elapsed":44,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class buffer_memory(object):\n","  # initializing buffer memory using deque\n","  def __init__(self, memory_size):\n","    self.memory_size = memory_size\n","    self.memory = deque(maxlen=self.memory_size)\n","\n","  # to store state, action and reward onto buffer\n","  def store(self, observation, action, reward, next_observation, done):\n","    observation = np.expand_dims(observation, 0)\n","    next_observation = np.expand_dims(next_observation, 0)\n","    self.memory.append([observation, action, reward, next_observation, done])\n","\n","  # to sample for training\n","  def sample(self, batch_size):\n","    sampl_batch = random.sample(self.memory, batch_size)                          # randomly sample set of state measures\n","    observations, actions, rewards, next_observations, dones = zip(*sampl_batch)\n","    observations = np.concatenate(observations, 0)                                # convert single state 2d obs array into batch size of 2d array\n","    next_observations = np.concatenate(next_observations, 0)\n","    return observations, actions, rewards, next_observations, dones\n","\n"],"metadata":{"id":"tcbzpeyDjoma","executionInfo":{"status":"ok","timestamp":1686740339323,"user_tz":-330,"elapsed":371,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def train(buffer, target_model, eval_model, gamma, optimizer, batch_size, loss_fn, count, update_freq):\n","  # collect a batch of random samples\n","  observations, actions, rewards, next_observations, dones = buffer.sample(batch_size)\n","\n","  # convert each arrays to tensor type\n","  observations = torch.FloatTensor(observations)\n","  actions = torch.LongTensor(actions)\n","  rewards = torch.FloatTensor(rewards)\n","  next_observations = torch.FloatTensor(next_observations)\n","  dones = torch.FloatTensor(dones)\n","\n","  q_vals = eval_model.forward_pass(observations)                                      # get Qt values (2d tensor of two element) for all observation (observation is 2d) from eval model\n","  next_q_vals = target_model.forward_pass(next_observations)                          # get Qt+1 values for all next observation from target model\n","  next_v_vals = target_model.get_V_val(next_q_vals)                                        # get Vt+1 values (2d tensor of single element) for all Qt+1 from target model\n","\n","  q_vals = q_vals.gather(1, actions.unsqueeze(1)).squeeze(1)                          # get back Vt values of all corresponding action\n","  expected_q_vals = rewards + gamma * (1 - dones) * next_v_vals.squeeze(-1)\n","\n","  # calculate loss\n","  loss = (expected_q_vals.detach() - q_vals).pow(2)\n","  loss = loss.mean()\n","\n","  optimizer.zero_grad()         # set eval_model gradient to none\n","  loss.backward()               # computes the gradient w.r.t loss\n","  optimizer.step()              # Performs a single optimization step (parameter update)\n","\n","  if count % update_freq == 0:  # update target model for every 200 steps by sharing the params of eval model\n","    target_model.load_state_dict(eval_model.state_dict())\n","\n","  return loss"],"metadata":{"id":"wCsWu9eQjwUt","executionInfo":{"status":"ok","timestamp":1686740357496,"user_tz":-330,"elapsed":571,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["## start here\n","\n","gamma = 0.99             # discount rate\n","learning_rate = 0.0001   # learning rate\n","batch_size = 32          # training batch size\n","update_freq = 200        # update target network for every 200 steps (after every 200 state)\n","capacity = 5000   # size of buffer memory\n","render = False           # renedering of cartpole window\n","episode = 500    # Total episode\n","alpha = 4                # entropy/temperature coefficient\n","\n","env = gym.make('CartPole-v1')\n","env = env.unwrapped\n","observation_dim = env.observation_space.shape[0]    # State space size: 4\n","action_dim = env.action_space.n                     # Action space size: 2\n","print(observation_dim, '||', action_dim)\n","\n","target_net = Soft_Q_Net(observation_dim, action_dim, alpha)   # initializing target nn\n","eval_net = Soft_Q_Net(observation_dim, action_dim, alpha)     # initializing evaluation nn\n","eval_net.load_state_dict(target_net.state_dict())             # loading initialized params (weights and biases) of target nn to eval nn\n","#print(target_net.state_dict()['FC1.weight'].shape)\n","#print(target_net.state_dict()['FC2.weight'].shape)\n","\n","optimizer = torch.optim.Adam(eval_net.parameters(), lr=learning_rate)   # optimizer\n","buffer = buffer_memory(capacity)                                        # initialize buffer memory\n","loss_fn = nn.MSELoss()                                                  # to measures the mean squared error\n","\n","count = 0\n","weight_reward = None\n","\n","for i in range(episode):\n","  # within each episode\n","  obs = env.reset()     # get initial state observation from env\n","  reward_total = 0      # total reward got in a episode\n","  if render:\n","    env.render()\n","  while True:\n","    obs_2d_tensor = torch.FloatTensor(np.expand_dims(obs, 0))               # convert 1d array to 2d array then to float tensor\n","    q_val, v_val, pi_maxent, action = eval_net.act(obs_2d_tensor)           # selecting action from agent sampling distribution\n","    #print(q_value, '||', v, '||', pi_maxent, '||', dist, '||', action, '\\nTotal reward: ', reward_total, 'count: ', count, '\\n-------------------------------------\\n')\n","    count += 1                                                              # indicate number of state agent covered till present\n","    next_obs, reward, done, info, _ = env.step(action)                      # taking sampled action on environment\n","    buffer.store(obs, action, reward, next_obs, done)                       # storing the st, at, rt+1, st+1 into buffer\n","    reward_total += reward                                                  # incrementing episode reward with current state reward\n","    obs = next_obs                                                          # set next state as current state\n","    if render:\n","      env.render()\n","    if len(buffer.memory) > batch_size:                                     # if buffer have more new samples than batch size (32); trainig will be done\n","      loss = train(buffer, target_net, eval_net, gamma, optimizer, batch_size, loss_fn, count, update_freq)\n","    if done:\n","      if not weight_reward:\n","        weight_reward = reward_total\n","      else:\n","        weight_reward = 0.99 * weight_reward + 0.01 * reward_total          # a relative current episode reward with past episodes reward\n","      if (i+1) % 10 == 0:\n","        print('episode: {}\\treward: {}\\tweight_reward: {:.3f}\\tepisode loss: {:.3f}'.format(i+1, reward_total, weight_reward, loss))\n","      break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wKbMSDt3j0uY","executionInfo":{"status":"ok","timestamp":1686741267161,"user_tz":-330,"elapsed":266688,"user":{"displayName":"Lukmanul Hakeem M","userId":"17183038146671070294"}},"outputId":"a839af21-bf7e-466f-f193-df94c075eccc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["4 || 2\n","episode: 10\treward: 18.0\tweight_reward: 16.647\tepisode loss: 2.866\n","episode: 20\treward: 14.0\tweight_reward: 17.944\tepisode loss: 7.887\n","episode: 30\treward: 12.0\tweight_reward: 18.242\tepisode loss: 16.446\n","episode: 40\treward: 27.0\tweight_reward: 18.093\tepisode loss: 22.876\n","episode: 50\treward: 45.0\tweight_reward: 18.670\tepisode loss: 30.261\n","episode: 60\treward: 20.0\tweight_reward: 19.122\tepisode loss: 27.133\n","episode: 70\treward: 10.0\tweight_reward: 19.552\tepisode loss: 43.906\n","episode: 80\treward: 22.0\tweight_reward: 19.807\tepisode loss: 17.019\n","episode: 90\treward: 23.0\tweight_reward: 19.682\tepisode loss: 2.126\n","episode: 100\treward: 56.0\tweight_reward: 21.004\tepisode loss: 10.332\n","episode: 110\treward: 24.0\tweight_reward: 22.387\tepisode loss: 98.754\n","episode: 120\treward: 22.0\tweight_reward: 23.663\tepisode loss: 4.418\n","episode: 130\treward: 36.0\tweight_reward: 24.187\tepisode loss: 46.142\n","episode: 140\treward: 26.0\tweight_reward: 25.934\tepisode loss: 52.887\n","episode: 150\treward: 13.0\tweight_reward: 27.314\tepisode loss: 48.903\n","episode: 160\treward: 47.0\tweight_reward: 30.171\tepisode loss: 5.312\n","episode: 170\treward: 29.0\tweight_reward: 31.228\tepisode loss: 7.260\n","episode: 180\treward: 40.0\tweight_reward: 34.181\tepisode loss: 52.599\n","episode: 190\treward: 29.0\tweight_reward: 35.926\tepisode loss: 192.118\n","episode: 200\treward: 60.0\tweight_reward: 38.007\tepisode loss: 111.171\n","episode: 210\treward: 125.0\tweight_reward: 42.410\tepisode loss: 6.147\n","episode: 220\treward: 274.0\tweight_reward: 51.599\tepisode loss: 7.083\n","episode: 230\treward: 117.0\tweight_reward: 60.367\tepisode loss: 103.660\n","episode: 240\treward: 117.0\tweight_reward: 76.421\tepisode loss: 2.237\n","episode: 250\treward: 167.0\tweight_reward: 89.767\tepisode loss: 2.513\n","episode: 260\treward: 128.0\tweight_reward: 100.874\tepisode loss: 3.433\n","episode: 270\treward: 178.0\tweight_reward: 110.386\tepisode loss: 2.802\n","episode: 280\treward: 145.0\tweight_reward: 109.424\tepisode loss: 201.623\n","episode: 290\treward: 80.0\tweight_reward: 109.642\tepisode loss: 2.387\n","episode: 300\treward: 390.0\tweight_reward: 113.561\tepisode loss: 3.279\n","episode: 310\treward: 371.0\tweight_reward: 142.993\tepisode loss: 735.297\n","episode: 320\treward: 413.0\tweight_reward: 164.280\tepisode loss: 2.141\n","episode: 330\treward: 380.0\tweight_reward: 179.077\tepisode loss: 1352.942\n","episode: 340\treward: 423.0\tweight_reward: 201.629\tepisode loss: 6.504\n","episode: 350\treward: 299.0\tweight_reward: 203.153\tepisode loss: 4.011\n","episode: 360\treward: 635.0\tweight_reward: 223.273\tepisode loss: 2.716\n","episode: 370\treward: 289.0\tweight_reward: 259.088\tepisode loss: 6.813\n","episode: 380\treward: 543.0\tweight_reward: 278.172\tepisode loss: 3.019\n","episode: 390\treward: 263.0\tweight_reward: 310.686\tepisode loss: 1.459\n","episode: 400\treward: 70.0\tweight_reward: 295.824\tepisode loss: 11.714\n","episode: 410\treward: 121.0\tweight_reward: 278.420\tepisode loss: 3.533\n","episode: 420\treward: 216.0\tweight_reward: 264.519\tepisode loss: 4.583\n","episode: 430\treward: 237.0\tweight_reward: 258.367\tepisode loss: 3.336\n","episode: 440\treward: 3877.0\tweight_reward: 406.779\tepisode loss: 1.261\n","episode: 450\treward: 377.0\tweight_reward: 674.357\tepisode loss: 4.247\n","episode: 460\treward: 6385.0\tweight_reward: 900.196\tepisode loss: 0.338\n","episode: 470\treward: 408.0\tweight_reward: 1020.219\tepisode loss: 0.712\n","episode: 480\treward: 165.0\tweight_reward: 963.837\tepisode loss: 1.649\n","episode: 490\treward: 263.0\tweight_reward: 890.485\tepisode loss: 1.946\n","episode: 500\treward: 259.0\tweight_reward: 830.650\tepisode loss: 5.809\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"x_4WkcKYkNCd"},"execution_count":null,"outputs":[]}]}